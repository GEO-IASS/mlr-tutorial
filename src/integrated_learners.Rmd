# Integrated Learners

This page lists the learning methods already integrated in [%mlr].

Columns **Num.**, **Fac.**, **NAs**, and **Weights** indicate if a method can cope with
numerical and factor predictors, if it can deal with missing values in a meaningful way
(other than simply removing observations with missing values) and if observation
weights are supported.

Column **Props** shows further properties of the learning methods.
*ordered* indicates that a method can deal with ordered factor features.
For *classification*, you can see if binary and/or multi-class problems are supported
and if the learner accepts class weights.
For *survival analysis*, the censoring type is shown.
For example *rcens* means that the learning method can deal with right censored data.
Moreover, the type of prediction is displayed, where *prob* indicates that probabilities
can be predicted.
For *regression*, *se* means that additional to the mean response standard errors can be predicted.
See also [&RLearner] for details.


```{r include=FALSE}
library("mlr")
library("pander")
linkPkg = function(x) {
  collapse(sprintf("[%1$s](http://www.rdocumentation.org/packages/%1$s/)", x), sep = "<br />")
}

getTab = function(type) {
  lrns = listLearners(type, create = TRUE, warn.missing.packages = FALSE)
  cols = c("ID / Short Name", "Name", "Packages", "Num.", "Fac.", "NAs", "Weights", "Props", "Note")
  df = makeDataFrame(nrow = length(lrns), ncol = length(cols),
    col.types = c("character", "character", "character", "logical", "logical", "logical", "logical", "character", "character"))
  names(df) = cols

  cn = function(x) if (is.null(x)) NA else gsub("\\n", " ", x)

  for (i in seq_along(lrns)) {
    lrn = lrns[[i]]
    df[i, 1] = paste0("**",lrn$id,"** <br />", cn(lrn$short.name))
    df[i, 2] = cn(lrn$name)
    df[i, 3] = linkPkg(mlr:::cleanupPackageNames(lrn$package))
    df[i, 4] = hasLearnerProperties(lrn, "numerics")
    df[i, 5] = hasLearnerProperties(lrn, "factors")
    df[i, 6] = hasLearnerProperties(lrn, "missings")
    df[i, 7] = hasLearnerProperties(lrn, "weights")
    df[i, 8] = collapse(sort(setdiff(lrn$properties, c("numerics", "factors", "missings", "weights"))), sep = "<br />")
    df[i, 9] = cn(lrn$note)
  }
  logicals = vlapply(df, is.logical)
  df[logicals] = lapply(df[logicals], function(x) ifelse(x, "X", ""))
  df
}

makeTab = function(df) {
  pandoc.table(df, style = "rmarkdown", split.tables = Inf, split.cells = Inf,
    justify = c("left", "left", "left", "center", "center", "center", "center", "left", "left"))
}

types = c("classif", "multilabel", "regr", "surv", "cluster")
tables = lapply(types, getTab)
names(tables) = types
numbers = sapply(tables, nrow)
```


### Classification (`r numbers["classif"]`)
```{r echo=FALSE,results="asis"}
makeTab(tables[["classif"]])
```

### Regression (`r numbers["regr"]`)
```{r echo=FALSE,results="asis"}
makeTab(tables[["regr"]])
```

### Survival analysis (`r numbers["surv"]`)
```{r echo=FALSE,results="asis"}
makeTab(tables[["surv"]])
```

### Cluster analysis (`r numbers["cluster"]`)
```{r echo=FALSE,results="asis"}
makeTab(tables[["cluster"]])
```

### Cost-sensitive classification
For *ordinary misclassification costs* you can use all the standard classification methods listed
above.

For *example-dependent costs* there are several ways to generate cost-sensitive learners from
ordinary regression and classification learners.
See section [cost-sensitive classification](cost_sensitive_classif.md) and the documentation
of [&makeCostSensClassifWrapper], [&makeCostSensRegrWrapper] and [&makeCostSensWeightedPairsWrapper]
for details.

### Multilabel classification (`r numbers["multilabel"]`)
```{r echo=FALSE,results="asis"}
makeTab(tables[["multilabel"]])
```

Moreover, you can use the binary relevance method to apply ordinary classification learners
to the multilabel problem. See the documentation of function [&makeMultilabelBinaryRelevanceWrapper]
and the tutorial section on [multilabel classification](multilabel.md) for details.
