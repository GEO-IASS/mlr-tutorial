# Configure mlr

If you really know what you are doing you may think [%mlr] is limiting you in certain ways.
[%mlr] is designed to make usage errors due to typos or invalid parameter values
as unlikely as possible.
But sometimes you want to break those barriers and get full access.
For all parameters, simply refer to the documentation of [&configureMlr].


## Example: Reduce the output on the console

You are bothered by all the output on the console like in this example?

```{r}
## Perform a 3-fold cross-validation
rdesc = makeResampleDesc("CV", iters = 3)
r = resample("classif.ksvm", iris.task, rdesc)
```

Just try the following:

```{r}
configureMlr(show.learner.output = FALSE)
rdesc = makeResampleDesc("CV", iters = 3)
r = resample("classif.ksvm", iris.task, rdesc)
```


## Access the current configuration

Function [&getMlrOptions] returns a ``list`` that shows the current configuration:

```{r}
getMlrOptions()
```


## Example: Turn off parameter checking

Or maybe you want to access a new parameter of a [Learner](&makeLearner) where the learner
is already available in [%mlr], but the parameter is not "registered" in the learner's parameter set yet.
If this is the case you might want to [contact us](https://github.com/mlr-org/mlr#get-in-touch)
or [open an issue](https://github.com/mlr-org/mlr/issues/new) as well!
But until then you can turn off [%mlr]'s parameter checking like this:

```{r error=TRUE}
lrn = makeLearner("classif.ksvm", newPar = 3)
lrn = makeLearner("classif.ksvm", epsilon = -3)

configureMlr(on.par.without.desc = "quiet")
lrn = makeLearner("classif.ksvm", newPar = 3)
lrn = makeLearner("classif.ksvm", epsilon = -3)
```

The parameter setting will then be passed to the underlying function without further ado.


## Example: Handle errors in an underlying learning method

Another common situation is that a particular learning method throws an error.
The default behavior of [%mlr] is to generate an exception as well.
However, in some situations, for example if you conduct a [benchmark study](benchmark_experiments.md)
with multiple data sets and learners, you normally do not want to terminate the whole
benchmark experiment due to one error.
The following example shows how to prevent this:

```{r error=TRUE}
## This call gives an error caused by the low number of observations in class `virginica`
train("classif.qda", task = iris.task, subset = 1:104)

configureMlr(on.learner.error = "warn")
mod = train("classif.qda", task = iris.task, subset = 1:104)
mod

## mod is an object of class FailureModel
isFailureModel(mod)

## Get the error message
getFailureModelMsg(mod)

## NAs are predicted
predict(mod, iris.task)
```

Instead of an exception, a warning is issued and a [&FailureModel] is created that predicts
``NA``s for all new observations. Function [&getFailureModelMsg] extracts the error
message.
