# Regression
For the regression use case we use the well-known [Boston Housing](&mlbench::BostonHousing) dataset. The road map is as follows:

* define the learning task  ([here](task.md)),
* tune the model ([here](tune.md)),
* conduct a benchmark experiment ([here](benchmark_experiments.md)) and
* evaluate the performance of the model ([here](performance.md)).

First, let's have a look at the data.
```{r}
data(BostonHousing, package = "mlbench")
summary(BostonHousing)
```
This data set concerns housing in the suburban area of Boston. The target variable, chosen for the regression task, is ``medv`` - the median value of owner-occupied homes in $\$1000$'s. Description of the others $13$ attributes can be found [here](&mlbench::BostonHousing).

### Define a task
Now, let us continue with defining the regression [task](task.md).

```{r}
## Make a task
regr.task = makeRegrTask(data = BostonHousing, target = "medv")
regr.task
```
In order to get an overview of the features type, we can print out the ``regr.task``. This shows that there are $12$ numeric and one factor variables in the data set.

### Tuning
By calling ``listLearners("regr")`` we can see which learners are available for the regression task. We picked the classical linear regression model (``regr.lm``), the [Lasso](&penalized::penalized) regression model (``regr.penalized.lasso``), [SVM](&kernlab::ksvm) with a radial basis kernel (``regr.ksvm``) and random forest from the [ranger](&ranger::ranger) package (``regr.ranger``), which we would like to compare. Before setting up a [benchmark experiment](benchmark_experiments.md) we can specify which hyperparameters are going to be tuned. The [%mlr] package provides powerful tuning algorithms. See [Tuning](tuning.md) and [Advanced Tuning](advanced_tune.md) for more details.

For each learner one hyperparameter will be tuned, i.e. the Lasso penalty parameter ``lambda1``, kernel parameter ``sigma`` for SVM model and the number of trees (``num.trees``) in the random forest model. We start with specifying a search space for each of these parameters. With [&makeTuneControlRandom] we set the tuning method to be random search. Afterwards we take $5$-fold cross validation as our [resampling strategy](resample.md) and root mean squared error (``rmse``) as optimization criterion. Finally, we make tuning wrapper for each learner.

```{r}
set.seed(1234)

## Define a search space for each learner'S parameter
ps_lasso = makeParamSet(
  makeNumericParam("lambda1", lower = 0, upper = 20)
)

ps_ksvm = makeParamSet(
  makeNumericParam("sigma", lower = -12, upper = 12, trafo = function(x) 2^x)
)

ps_rf = makeParamSet(
  makeIntegerParam("num.trees", lower = 1L, upper = 200L)
)

## Choose a resampling strategy
rdesc = makeResampleDesc("CV", iters = 5L)

## Choose a performance measure
meas = rmse

## Choose a tuning method
ctrl = makeTuneControlRandom(maxit = 100L)

## Make tuning wrappers
tuned.lasso = makeTuneWrapper("regr.penalized.lasso", rdesc, meas, ps_lasso, ctrl)
tuned.ksvm = makeTuneWrapper("regr.ksvm", rdesc, meas, ps_ksvm, ctrl)
tuned.rf = makeTuneWrapper("regr.ranger", rdesc, meas, ps_rf, ctrl)
```

### Benchmark Experiment
In order to conduct a [benchmark experiment](benchmark_experiments.md), it is necessary to choose an evaluation method. We will use the resampling strategy and the performance measure from the previous section and then pass the tuning wrappers as arguments into the [&benchmark] function.

```{r}
## Four learners to be compared
lrns = list(makeLearner("regr.lm"), tuned.ksvm, tuned.lasso, tuned.rf)


## Conduct the benchmark experiment
bmr = benchmark(lrns, regr.task, rdesc, measures = rmse, show.info = FALSE)
```


### Performance

Now we want to evaluate the results.
```{r}
getBMRAggrPerformances(bmr)
```

A closer look at the boxplot reveals that random forest outperforms the other learners for this specific task. Despite the tuning procedure performed before, the benchmark experiment for linear and lasso regression yields similar but poor results.

```{r}
plotBMRBoxplots(bmr)
```

