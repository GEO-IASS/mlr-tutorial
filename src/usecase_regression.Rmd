# Regression
For the regression use case we use the well-known [Boston Housing](https://www.rdocumentation.org/packages/mlbench/versions/0.4-1/topics/BostonHousing) dataset. The road map is as follows:

* define the learning task  ([here](http://mlr-org.github.io/mlr-tutorial/devel/html/task/index.html)),
* tune the model ([here](http://mlr-org.github.io/mlr-tutorial/devel/html/tune/index.html)),
* conduct a benchmark experiment ([here](http://mlr-org.github.io/mlr-tutorial/devel/html/benchmark_experiments/index.html)) and
* evaluate the performance of the model ([here](http://mlr-org.github.io/mlr-tutorial/devel/html/performance/index.html)).

First, we have a look at a part of the data.
```{r}
data(BostonHousing, package = "mlbench")
head(BostonHousing)
```
We have 14 numeric variables and as before ``medv`` is chosen to be the response variable for the regression model. 

### Define a task
Let's define the regression [task](task.md).

```{r}
## Make a task
regr.task = makeRegrTask(data = BostonHousing, target = "medv")
regr.task$task.desc$n.feat
```
With the command ``regr.task$task.desc$n.feat`` the type of all variables in the data can be accessed. 

### Tuning
By calling ``listLearners("regr")`` we can see which learners are available for the regression task. We picked the classical linear regression model (``regr.lm``), the [Lasso](https://www.rdocumentation.org/packages/penalized/versions/0.9-50) regression model (``regr.glmnet``), [SVM](https://www.rdocumentation.org/packages/kernlab/versions/0.9-25) with a radial basis kernel (``regr.ksvm``) and random forest from the [ranger](https://www.rdocumentation.org/packages/ranger/versions/0.6.0) package (``regr.ranger``), which we would like to compare. Before setting up the [benchmark experiment](benchmark_experiments.md) some of the models' hyperparameters can be tuned. The [%mlr] package provides powerful tuning algorithms. See [Tuning](tuning.md) and [Advanced Tuning](advanced_tune.md) for more details.

We start with specifying the search space for each parameter. Note that for all learners the hyperparameters, which will be tuned, should be positive. With [&makeTuneControlRandom] we set the tuning method to be random search. Afterwards we take $10$-fold cross validation as our [resampling strategy](resample.md) and root mean squared error (``rmse``) as the optimization criterion.

```{r}
set.seed(1234)

## Define the learners for the tuning
base.learners = list(
  makeLearner("regr.ksvm"),
  makeLearner("regr.ranger"),
  makeLearner("regr.penalized.lasso")
)
lrn = makeModelMultiplexer(base.learners)

## Define a search space for each parameter
ps = makeModelMultiplexerParamSet(lrn,
  makeNumericParam("sigma", lower = -12, upper = 12, trafo = function(x) 2^x),
  makeIntegerParam("num.trees", lower = 1L, upper = 700L),
  makeNumericParam("lambda1",  lower = 0, upper = 20)
  
)

## Choose a resampling strategy
rdesc = makeResampleDesc("CV", iters = 10L)

## Choose a performance measure
meas = rmse

## Choose a tuning method
ctrl = makeTuneControlRandom(maxit = 300L)

## Tune the parameters
res = tuneParams(lrn, task = regr.task, resampling = rdesc, par.set = ps,
                                measures = meas, control = ctrl, show.info = FALSE)
## Extract the results
res.df = as.data.frame(trafoOptPath(res$opt.path))
head(res.df)

## Extract the optimal parameter values for each learner
res.lasso = res.df[(res.df$selected.learner=="regr.penalized.lasso"), c("regr.penalized.lasso.lambda1", "rmse.test.rmse")]
lambda1 = list("lambda1" = res.lasso$regr.penalized.lasso.lambda1[which.min(res.lasso[, 2])])

res.svm = res.df[(res.df$selected.learner=="regr.ksvm"), c("regr.ksvm.sigma", "rmse.test.rmse")]
sigma = list("sigma" = res.svm$regr.ksvm.sigma[which.min(res.svm[, 2])])

res.rf = res.df[(res.df$selected.learner=="regr.ranger"), c("regr.ranger.num.trees", "rmse.test.rmse")]
num.trees = list("num.trees" = res.rf$regr.ranger.num.trees[which.min(res.rf[, 2])])
```
Finally, we extracted the optimal parameter value for each learner. Now we are ready to proceed with the benchmark experiment!

### Benchmark Experiment
In order to conduct a [benchmark experiment](benchmark_experiments.md), it is necessary to choose an evaluation method. We will use the resampling strategy from the previous section and two performance measures - root mean squared error (``rmse``) and mean of absolute error (``mae``).

```{r}
## Four learners to be compared
lrns = list(makeLearner("regr.lm"), 
            makeLearner("regr.ksvm", par.vals = sigma),
            makeLearner("regr.ranger", par.vals = num.trees),
            makeLearner("regr.penalized.lasso", par.vals = lambda1))

## Conduct the benchmark experiment
bmr = benchmark(lrns, regr.task, rdesc, measures = list(rmse, mae), show.info = FALSE)
```


### Performance

Now we want to evaluate the results.
```{r}
head(getBMRPerformances(bmr, as.df = TRUE))

```

A closer look at the first boxplot reveals that random forest outperforms the other learners for this specific task. However, the SVM model using the measure ``mae`` seems to perform better than the other models. Moreover, in both cases the benchmark experiment for the linear regression and lasso regression models yield similar but poor results.

```{r}
plotBMRBoxplots(bmr, measure = rmse)
plotBMRBoxplots(bmr, measure = mae)
```

