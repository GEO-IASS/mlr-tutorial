# Integrating another Measure

In some cases, you may want to evaluate a [&Prediction] with a [Measure](&makeMeasure) which is not yet
implemented in [%mlr]. This could be either a performance measure which is not included in
the in [%mlr] [list of measures](measures.md) or a measure that uses a non-standard misclassification
cost matrix.


## Construct a performance measure

The [&makeMeasure] function provides a simple way of constructing your own performance
measure. Below, this is exemplified by an implementation of the mean
misclassification error ([mmce](measures.md)) for the [iris](&base::iris) dataset. We write a simple
function that computes the measure on the basis of the predictions and subsequently wrap it
in a [Measure](&makeMeasure) object. Then, we work with it as usual with the
[&performance] function.
See the **R** documentation of the [&makeMeasure] function for details on the
various parameters.

```{r}
## Define the measure
my.mmce.fun = function(task, model, pred, feats, extra.args) {
  tb = table(getPredictionResponse(pred), getPredictionTruth(pred))
  1 - sum(diag(tb)) / sum(tb)
}

## Encapsulate the function with a Measure object
my.mmce = makeMeasure(id = "my.mmce", minimize = TRUE,
  properties = c("classif", "classif.multi", "req.pred", "req.truth"),
  fun = my.mmce.fun, best = 0, worst = 1, name = "My Mean Misclassification Error")

## Create classification task and learner
m = train("classif.lda", iris.task)
pred = predict(m, task = iris.task)

## Compare predicted and true label with our measure
performance(pred, measures = my.mmce)

## Apparently the result coincides with the mlr implementaion
performance(pred, measures = mmce)
```


## Construct a measure for non-standard misclassification costs

To create a measure that involves non-standard misclassification costs you can use
the [&makeCostMeasure] function. In order to do this, you first need to define the cost
matrix you want to use and include all class labels. The cost matrix can then be
wrapped in a [Measure](&makeMeasure) object and a prediction can be evaluated as usual with the
[&performance] function. See the **R** documentation of the [&makeCostMeasure] function for
details on the various parameters.

```r
## Create misclassification cost matrix
mcm = matrix(c(0, 2, 2, 3, 0, 2, 1, 1, 0), ncol = 3)
rownames(mcm) = colnames(mcm) = getTaskClassLevels(iris.task)

## Create classification task and learner
m = train("classif.lda", iris.task)
pred = predict(m, newdata = iris)

## Encapsulate the cost matrix in a Measure object
my.costs = makeCostMeasure(id = "costs", minimize = TRUE, costs = mcm, task = iris.task,
  best = 0, worst = 3, name = "My Costs")

## Compare predicted and true label with our measure
performance(pred, measures = my.costs)
```


## Create an aggregation function

It is possible to create your own aggregation function by calling the
[%mlr] function [&makeAggregation].

```{r}
makeAggregation(id = "some.id", name = "some name",
  fun = function (task, perf.test, perf.train, measure, group, pred) {
    ## stuff you want to do with perf.test or perf.train
  }
)
```

Remember: It is important that the head of the function looks exactly as above!
`perf.test` and `perf.train` are both numerical vectors containing the preformances on
train and test sets.
In the usual case (e.g. cross validation), the `perf.train` vector is empty.


### Example: Evaluate the range of measures

Let's say you are interested in the range of the obtained measures:
```{r}
my.range.aggr = makeAggregation(id = "test.range", name = "Test Range",
  fun = function (task, perf.test, perf.train, measure, group, pred)
    diff(range(perf.test))
)
```

Now we can run a feature selection based on the first measure in the provided
list and see how the other measures turn out.
```{r}
ms1 = mmce
ms2 = setAggregation(ms1, my.range.aggr)
ms1min = setAggregation(ms1, test.min)
ms1max = setAggregation(ms1, test.max)
rdesc = makeResampleDesc("CV", iters = 3)
res = selectFeatures("classif.rpart", iris.task, rdesc, measures = list(ms1, ms2, ms1min, ms1max),
  control = makeFeatSelControlExhaustive(), show.info = FALSE)
perf.data = as.data.frame(res$opt.path)
p = ggplot(aes(x = mmce.test.mean, y = mmce.test.range, xmax = mmce.test.max, xmin = mmce.test.min,
  color = as.factor(Sepal.Width), pch = as.factor(Petal.Width)), data = perf.data) +
  geom_point(size = 4) +
  geom_errorbarh(height = 0)
print(p)
```
