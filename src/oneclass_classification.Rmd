# One-Class Classification

*Regular classification* or *cost-sensitive classification* are supervised learning methods, *class labels are given* for each observation and are used to train the model. Classification methods, therefore, are provided with the information of which observations belongs to which class and thus can learn class patterns and apply those onto new data. To learn the patterns for the classes a sufficient number of observations in each class is required and (nearly) balanced class sizes.

But what if there are no labels in the data?

And what if the class sizes a unbalanced?

And what if the classes are not just unbalanced, but one class has very a small number of observations?

If you have all of those cases in your data, you probably want to apply *Anomaly Detection methods*.

An *anomaly* is "something that deviates from what is standard, normal, or expected"
[Oxford English Dictionary]. In the sense of data, anomalies are observations, that deviates from the majority
of the data [Amer et al., 2013, p.1]. In different application domains anomalies are
also referred to as *outliers*, *discordant observations*, *exceptions*, *aberrations*, *surprises*, *peculiarities*, *contaminants* [Chandola et al., 2009, p.1], *novelties*, *noise* or *deviations* [Dau et al., 2014, p.2].

There are two ways of handling anomalies,depending on the type of the anomaly:  either delete them to yield statistically
significant increase in accuracy [Smith and Martinez, 2011, p.1] or translate them into significant actionable information [Chandola et al., 2009, p.1] [Hodge and Austin, 2004,
p.1] [Dau et al., 2014, p.1]. An example for the former case would be anomalies caused
by measurement errors, which one want to remove from the data. For the latter case, a typical example is anomalous traffic pattern in a computer network, which could be interpreted as a hacker attack [Kumar., 2005] and which one obviously want to prevent. Whereas methods to find anomalies to extract relevant information are referred to as *anomaly detection*.

A special case of *Anomaly Detection* is the *one-class classification*. As the name *one-class learning* already says, the problem is that most of the time we only observe measurements/data of one class, namely the *normal* class. Especially in real world scenarios, it is easy to collect the normal situation, for example, production machines are probably nearly always run as it should, aircraft machinery works like it should and so on. It is hard to simulate the situation which is not normal, and even if we would be able to simulate this situation, we wouldn’t know if we cover all possible situations of anomalies, since anomalies can be of diverse nature.

## One-class classification approach

As described in the previous section *one-class classification* is a *semi-supervised* learning approach. A requirement to apply *one-class classification* methods is to have training data, that consist only only consists of *normal* observations (therefore *semi-supervised*). The idea is to learn how the data is structured if the data is from the normal class. If the method learned the structure well it can tell in new data if an anomalous observation deviates from the normal class.
Unfortunately, in real life settings, there is usually no way to test, if any or all anomalies were detected, as usually labels are not given. In [%mlr] a new class is introduced to handle anomalies and also to test the methods, in case benchmark data are used or labels do exist. This necessary to provide reproducibility of the experiements and comparability of the methods.

## Quick start
Here we show the [%mlr] workflow to train, make predictions, and evaluate a learner on a one-class classification problem. More details on each step are provided later.

```{r, results='hide', warning = FALSE}
devtools::install_github("mlr-org/mlr", ref = "oneclass_lof")
library(mlr)
# get synthetic anomaly data, first 1000 observation are normal, the last 50 are anomalies
# this dataset has labels in the 'target' column for testing the model
data = getTaskData(oneclass2d.task)
train.set = 1:550
test.set = 551:1050

## 1) Define the task
## For one-class classification, unlike other learning problem, the label names of the 
## positive/anomaly and negative/normal class needs to be provided.
## Note, the target column is not used for training only for evaluating
task = makeOneClassTask(id = "oneclass", data = data, target = "Target", positive = "Anomaly", negative = "Normal")

## 2) Define the learner
## Currently four methods are implemented for one-class classification
## All learner can have the predicted type response or probability
## per default the predict.threshold is set to the 95%-quantile, but if the user have information about
## the ratio of anomalies in the data, he/she should set predict.type to "prob" and adapt the predict.threshold variable
lrn.ksvm = makeLearner("oneclass.ksvm", predict.type = "response")
lrn.svm = makeLearner("oneclass.svm", predict.type = "response")
lrn.lof = makeLearner("oneclass.lofactor", predict.type = "prob", k = 60, predict.threshold = 0.95)
## sometimes the h2o learner are not computational stable, therefore add learner settings,
## e.g. l1, max_w2 etc.
lrn.ae = makeLearner("oneclass.h2o.autoencoder", predict.type = "prob", predict.threshold = 0.95, 
  activation = "Tanh", reproducible = TRUE, l2 = 1e-5, sparse = TRUE, max_w2 = 10)

## 3) Fit the model
## The model is fitted ignoring that labels are provided (more information later)
mod.ksvm = train(lrn.ksvm, task, subset = train.set)  
mod.svm = train(lrn.svm, task, subset = train.set)  
mod.lof = train(lrn.lof, task, subset = train.set)  
mod.ae = train(lrn.ae, task, subset = train.set)  

## 4) Make predictions
## If predict.type = "prob", observation with high probability are more likely to be anomalous
pred.ksvm = predict(mod.ksvm, task, subset = test.set)
pred.svm = predict(mod.svm, task, subset = test.set)
pred.lof = predict(mod.lof, task, subset = test.set)
pred.ae = predict(mod.ae, task, subset = test.set)

## 5) Evaluate the learner
## The provided labels are now used to evaluate the models above using measurement for binary classification
performance(pred.ksvm, measures = list(bac, f1, mmce))

## additional measurement for evaluating anomaly detection methods using labels are also implemented in R
## for using those measurement, create them first
rprecision = makePrecisionMeasure(id = "RPrecision", type = "rprecision", adjusted = FALSE)
precisionat5 = makePrecisionMeasure(id = "Precisionat5", p = 5, type = "precisionatp", adjusted = FALSE)
avgprecision = makePrecisionMeasure(id = "AvgPrecision", type = "avgprecision", adjusted = FALSE)
wac = makeWACMeasure(id = "wac", w = 0.6) #weight of the anomaly class is set to 0.6

performance(pred.svm, measures = list(rprecision, precisionat5, avgprecision, wac))
performance(pred.lof, measures = list(rprecision, precisionat5, avgprecision, wac))

## additional measurement for evaluating anomaly detection methods without using labels are also implemented in R
## for using those measurement, create them first (here for demonstration purpose set low n.alpha and n.sim)
## for dimension <= 8 use makeAMVMeasure
amv = makeAMVMeasure(id = "AMV", alphas = c(0.8, 0.99), n.alpha = 10, n.sim = 100)
## for dimension > 8 use makeAMVhdMeasure
amvhd = makeAMVhdMeasure(id = "AMV", alphas = c(0.8, 0.99), n.alpha = 10, n.sim = 100)

performance(model = mod.ae, pred = pred.ae, task = task, measures = list(amv))
```

If one only has a few data sets without labels, it is recommended to plot the probability and set the threshold manually.
```{r, out.width = '75%'}
par(mfrow = c(1,2))
n = length(pred.lof$data$prob.Anomaly)
plot(1:n, pred.lof$data$prob.Anomaly, main = "lof", ylab = "probability for anomaly", xlab = "observation")
plot(1:n, pred.ae$data$prob.Anomaly, main = "autoencoder", ylab = "probability for anomaly", xlab = "observation")
```

## Create a task

Usually, anomalous data sets don't have labels and therefore are an unsupervised learning problem, however, the [%mlr] one-class classification learner inherits from the supervised class. This ensures that all binary classification measures can be used for evaluating the model’s performance in case labeled data is given.

Before creating a [OneClassTask](&Task) problem you have to bring your data in the right format if it doesn't have a column containing the class labels. In case labels are missing create a synthetic column with only one class (e.g. name the column 'Target' and fill it with the label 'Normal'). 
The next step is to create a [OneClassTask](&Task), if you have the right format:
```{r}
data = getTaskData(oneclass2d.task)
train.set = 1:550
test.set = 551:1050

task = makeOneClassTask(id = "oneclass", data = data, target = "Target", positive = "Anomaly", negative = "Normal")
task
```
In contrast to other learning problem, you *always* need to specify which column is the label/target column and which label stands for the *anomaly* class (positive) and which for the *normal* class (negative). As already mentioned above one-class classification only trains on one-class, the normal class, so you need to tell [%mlr] what the other class is. 
In the following example, we used the synthetic created data from the already existing [oneclass2d.task](&oneclass2d.task) in [%mlr] for anomaly data, which has a label column.


## Constructing a learner
One-class classification in [%mlr] is currently provides three methods:
* One-Class Support Vector Machine

* Autoencoder (Artificial neural network)

* Local Outlier Factor (LOF)

All one-class classification learners in [%mlr] support response or probability output. The response output will state if the observation is predicted as *normal* or *anomaly*, more details on the response output will be specified when the learner is introduced below. For the probabilistic output in [%mlr], we are using *converted* anomaly scores. Anomaly scores are the usual output of anomaly detection methods indicating to what degree an observation is anomalous (relative to the other observations). The anomaly scores are constructed depending on the applied anomaly detection method. This can lead to different ranges while interpretation remains unchanged. An advantage of [%mlr] is the consistent structure of the package and to keep the consistency the scores output, returned by each learner are transformed to the range of [0,1]. Although this output is called *probability* within the [%mlr] package, it should not exactly be interpreted as a probability but more as a normalized score. For more information about the transformation see the help page of [&convertingScoresToProbability]. 

### Support vector machines
Support vector machines are well known for analyzing data for classification and regression analysis. SVMs can also apply to detect anomalies by "creating a spherical decision boundary around a set of [normal] data points"[Karatzoglou et al., 2004, p.9]. In **R** the package s [%e1071] and [%kernlab] are providing the algorithm to do so. 

```{r}
lrn.svm = makeLearner("oneclass.svm", predict.type = "prob")
lrn.ksvm = makeLearner("oneclass.ksvm")
lrn.ksvm
```
The anomaly scores, also called decision values, from both packages, are indicating the distance to the decision boundaries. Both packages are also returned a response output, which is received by thresholding of the decision values. The threshold is per default set to 0, or in other words scores are positive for normal points and negative for anomalies in [%e1071] and [%kernlab]. The lower the score the more likely the related observation is anomalous. When using the method in [%mlr], the output is automatically normalized to [0,1] as previously explained. And according to [&convertingScoresToProbability], after transformation $1-f(score)$ is used for the probability column.


### Autoencoder (Artificial neural network)
A special case of a neural network is an autoencoder. The main idea of an autoencoder is to set the output data identical to the input data in the neural network and to train the model to reconstruct the input, with the objective to minimize the reconstruction error [Dau et al.2014, p.314]. But this approach requires to only use the data with the normal class in the training, to make sure the networks learn the pattern of the normal class and therefore classifies previously unseen observations which match the normal pattern as normal otherwise as anomalies [Dau et al. 2014, p.311]. In **R** the package [%h2o] provides an algorithm for an autoencoder. In [%mlr] you can currently specify up to three layers, by first setting the number of layers in the *layer* parameter and for each layer the number of nodes with *nodes1*, *nodes2*, *nodes3*. 

```{r}
lrn.h2o = makeLearner("oneclass.h2o.autoencoder", par.vals = list(reproducible = T, seed = 1234), 
  predict.type = "prob", predict.threshold = 0.95, reproducible = TRUE, 
  layers = 2, nodes1 = 20, nodes2 = 10,  
  # following settings were made to stabilise the model
  activation = "Tanh", l2 = 1e-5, sparse = TRUE, max_w2 = 10)
lrn.h2o
```

As explained above, the autoencoder is producing a reconstruction error (mse between output and input layer in [&h2o]), which is the anomaly score. A high score or high mse means that the reconstruction error wasn't accurate, which tend to imply that the observation was rather anomalous. According to [&convertingScoresToProbability], after transformation $f(score)$ is used for the probability column. If you set the *predict.type* as *response*, [&mlr] applies a soft threshold of the 95%-quantile to the scores. Scores above the 95%-quantile are considered to be an anomaly.

### Local Outlier Factor (LOF)
text text. Unsupervised
```{r}
# The number of neighbours (k) that will be used in the calculation of the local 
# outlier factors must be set by the user
lrn.lof = makeLearner("oneclass.lofactor", k = 50, predict.type = "prob")
lrn.lof
```

The local outlier factor is returning a score indicating the dense of the local density (locality is defined by *k*). A low local density indicates that the observation lies in a region that is sparser than that of its neighbors, and thus it is more likely to be anomalous. According to [&convertingScoresToProbability], after transformation $f(score)$ is used for the probability column. If you set the *predict.type* as *response*, [&mlr] applies a soft threshold of the 95%-quantile to the scores. Scores above the 95%-quantile are considered to be an anomaly.

## Train
You can [&train] a model as usual with a oneclass learner and a oneclass task as input. You can pass a ``subset`` or a ``newdata set``.  
```{r, results='hide'}
mod.ksvm = train(lrn.ksvm, task, subset = train.set)  
mod.svm = train(lrn.svm, task, subset = train.set)
mod.lof = train(lrn.lof, task, subset = train.set)  
mod.ae = train(lrn.ae, task, subset = train.set) 
```

## Predict
Prediction can be done as usual in [%mlr] with [predict](&predict.WrappedModel) and by passing a trained model and either the task to the ``task`` argument or some new data to the ``newdata`` argument. As always you can specify a ``subset`` of the data which should be predicted. In section [Constructing a Learner)](oneclass_classification.md#constructing-a-learner), the specialities of the probability and response output are explained.
```{r, results='hide'}
pred.ksvm = predict(mod.ksvm, task, subset = test.set)
pred.svm = predict(mod.svm, task, subset = test.set)
pred.ae = predict(mod.ae, task, subset = test.set)
```

The training of LOF in [&mlr] is different than the other learners, as LOF is an unsupervised learning method. To apply LOF from the [&DMwR] package, no test and train data are needed. To keep the structure of mlr, to enable tuning and to allow binary supervised evaluation methods if labels are provided we adapt the [&lofactor] function to [&mlr]. That is that the (&train) and the (&test) function are independent. Which means that the (&train) function returns a prediction of the train data and the (&test) functions return a prediction of the test data based on the test data themselves, without needing the trained model. [&mlr] returns a message for the user.
```{r}
pred.lof = predict(mod.lof, task, subset = test.set)
```

## Performance
To assess the performance of the prediction, use the [&performance] function. As usually in [&mlr] you can specify with the `measures` argument which [measure(s)](measures.md) to calculate. The default measure for one-class classification is the ...(need to decide on!!). Use [&listMeasures] to see all available measures. More details can be found on the [table of performance measures](measures.md) and the [&measures] documentation page and in this section.

If the true labels are given you can use binary classification evaluation methods.
```{R}
performance(pred.ksvm, measures = list(bac, f1, mmce))
```


Binary classification evaluation methods consider the two classes as equal. However, in anomaly detection, the anomaly class is more interesting for the user. [&mlr] also implemented additional evaluation methods for evaluating anomaly detection methods in case true labels are provided. One method is *weighted accuracy (wac)* a modification of the *balanced accuracy (bac)*, for which the user needs to set the weight (in [0,1]) of the importance of anomaly class relative to the normal class. If the weight is set to 0.5 the *wac* equals the *bac*. *Bac* measure returns values between 0 and 1, whereas 1 indicates perfect prediction.
```{R}
wac = makeWACMeasure(id = "wac", w = 0.6) #weight of the anomaly class is set to 0.6
performance(pred.svm, measures = list(wac))
```

The other methods are *R-Precision measure/top p-accuracy*, *precision at p* and *average precision* (see [Campos et al., 2016]), which are specially aimed to evaluate anomaly detection models. For using this measures, you need to create them first with [&makePrecisionMeasure]. More information is in [Campos et al., 2016] and the help page [&makePrecisionMeasure]. All three measures return values between 0 and 1, whereas 1 indicates perfect prediction.
```{R}
## additional measurement for evaluating anomaly detection methods using labels are also implemented in R
## for using those measurement, create them first
rprecision = makePrecisionMeasure(id = "RPrecision", type = "rprecision", adjusted = FALSE)
precisionat5 = makePrecisionMeasure(id = "Precisionat5", p = 5, type = "precisionatp", adjusted = FALSE)
avgprecision = makePrecisionMeasure(id = "AvgPrecision", type = "avgprecision", adjusted = FALSE)

performance(pred.lof, measures = list(rprecision, precisionat5, avgprecision))
```

A key challenge in anomaly detection is, that usually no labels are available and in that case all introduced measures above are not applicable. For that case [&mlr] provides the *Area under the Mass-Volume Curve (AMV)* measurement according to [Thomas et al., 2016] for dimension smaller than eight. This measure needs be created with [&makeAMVMeasure]. For datasets with greater dimension than eight [&mlr] provides the *Area under the Mass-Volume Curve for high dimensional data (AMVhd)* according to [Goix, 2016], analogous use [&makeAMVhdMeasure] to create the measurement. More information is provided in the named paper and the help page [&makeAMVMeasure] and [&makeAMVhdMeasure].
```{R}
## additional measurement for evaluating anomaly detection methods without using labels are also implemented in R
## for using those measurement, create them first (here for demonstration purpose set low n.alpha and n.sim)
## for dimension <= 8 use makeAMVMeasure
amv = makeAMVMeasure(id = "AMV", alphas = c(0.8, 0.99), n.alpha = 10, n.sim = 100)
## for dimension > 8 use makeAMVhdMeasure
amvhd = makeAMVhdMeasure(id = "AMV", alphas = c(0.8, 0.99), n.alpha = 10, n.sim = 100)
performance(model = mod.ae, pred = pred.ae, task = task, measures = list(amv))
```


## Tuning/benchmark experiment
A challenge in anomaly detection is to set the parameters, which is not an easy task. If you have data with labels, you can tune a model using a supervised binary measurement and use this model on unlabelled data. If you don't have labeled data, you can tune a model using an unsupervised binary measurement like *amv* or *amvhd*. A tuning example for all implemented anomaly detection is provided below using the [oneclass2d.task](&oneclass2d.task).

### Defining the resampling strategy for one-class classification
For one-class classification tuning you can use the standard [resampling strategy] (resample.md#defining-the-resampling-strategy), especially if you don't have labels. But if you have labels it is recommended to use a resampling strategy designed for the one-class classification task, which are: ``OCHoldhout``, ``OCCV``, ``OCSubsample``, ``OCBootsrap``, ``OCRepCV``, to assure to only have normal data during the training. ``OCHoldhout``,``OCSubsample``, ``OCBootsrap`` are basically sampling only normal observations for the training data set and the rest are using for testing. ``OCCV`` and ``OCRepCV`` split the anomalous observations in k-folds, when a fold became the training set it drops the anomalies for training. These strategies can be called as usual in [&mlr] with the [&makeResampleDesc] function.
```{r}
# Set outer resampling strategy
outer = makeResampleDesc("OCBootstrap", iters = 3)
rin = makeResampleInstance(outer, oneclass2d.task)

# Set inner resampling strategy
inner = makeResampleDesc("CV", iters = 3)
```

### Specifying the optimization algorithm
For tuning an anomaly detection learner, you need to always create a learner with a ``prob`` output. Because, if you want to tune an anomaly learner using
*1.  a supervised measurement (e.g. ``f1`` or ``average precision``), you should tune the threshold, as those measures based on the number of correct predicted anomalies or true predicted normal points.

*2. an unsupervised measurement like ``avm`` or ``amvhd``, no threshold is needed, but the algorithm of those measures are based on an anomaly scores, which is the probability output in [&mlr]
```{r}
# Set search strategy (maxit is low to run this example)
ctrl = makeTuneControlRandom(maxit = 5L, tune.threshold = TRUE)
```
 
### Performing the benchmark experiment

For conducting a benchmark experiment with three learners, you need to specify the search space for each learner. The procedure is described in [Tuning](tune.md) and [Benchmark](benchmark_experiments.md) section.  
```{r}
### parameter for svm
ps.svm = makeParamSet(
  makeNumericParam("nu",  lower = 0.01, upper = 0.99),
  makeDiscreteParam("kernel", values = c("linear", "polynomial", "radial", "sigmoid")),
  makeIntegerParam("degree", lower = 1L, upper = 4L, requires = quote(kernel == "polynomial")),
  makeNumericParam("coef0", lower = 0, upper = 3, requires = quote(kernel == "polynomial" || kernel == "sigmoid"))
)

### parameter for lofactor
ps.lofactor = makeParamSet(
  makeIntegerParam("k", lower = 20, upper = 200))
```
For the *oneclass.ksvm* learner there is an exception for setting the search space for the ``sigma`` parameter, see [Tuning](tune.md#specifying-the-search-space).
```{r}
### parameter for ksvm
ps.ksvm = makeParamSet(
  makeNumericParam("nu",  lower = 0.01, upper = 0.99),
  makeDiscreteParam("kernel", values = c("vanilladot", "polydot", "rbfdot", "tanhdot")),
  makeIntegerParam("degree", lower = 1L, upper = 4L, requires = quote(kernel %in% c("polydot", "anovadot", "besseldot"))),
  # for sigma 
  makeNumericParam("sigma", lower = -10, upper = 10, trafo = function(x) 10^x, require = quote(kernel %in% c("rbfdot", "anovadot", "besseldot", "laplacedot"))),
  makeNumericParam("scale", lower = 0, upper = 1, requires = quote(kernel %in% c("polydot", "tanhdot"))),
  makeNumericParam("offset", lower = 0, upper = 3, requires = quote(kernel %in% c("polydot", "tanhdot"))),
  makeIntegerParam(id = "order", lower = 1, upper = 3, requires = quote(kernel == "besseldot"))
)
```
There is also an exception fot the *oneclass.h2o.autoencoder*. [&mlr] enables to tune for number of layers and nodes up to three layers.
```{r}
### parameter for autoencoder
ps.h2o = makeParamSet(
  makeIntegerParam(id = "layers", lower = 1L, upper = 3L, default = 1L),
  makeIntegerParam(id = "nodes1", lower = 150L, upper = 250, default = 200L),
  makeIntegerParam(id = "nodes2", lower = 150L, upper = 250, default = 200L,
    requires = quote(layers > 1)),
  makeIntegerParam(id = "nodes3", lower = 150L, upper = 250, default = 200L,
    requires = quote(layers > 2))
)
```

After defining the search space for the learners, set the learners and apply the tuning wrapper like section [Benchmark Experiment](benchmark_experiments.md#tuning).
```{r}
# make lofactor learner and tunewrapper
lrn.svm = makeLearner("oneclass.svm", predict.type = "prob")
lrn.svm = makeTuneWrapper(lrn.svm, resampling = inner, par.set = ps.svm, measures = list(f1),
  control = ctrl, show.info = FALSE)

# make lofactor learner and tunewrapper
lrn.ksvm = makeLearner("oneclass.ksvm", predict.type = "prob")
lrn.ksvm = makeTuneWrapper(lrn.ksvm, resampling = inner, par.set = ps.ksvm, measures = list(f1),
  control = ctrl, show.info = FALSE)

# make lofactor learner and tunewrapper
lrn.lofactor = makeLearner("oneclass.lofactor", predict.type = "prob")
lrn.lofactor = makeTuneWrapper(lrn.lofactor, resampling = inner, par.set = ps.lofactor, measures = list(f1),
 control = ctrl, show.info = FALSE)

# make autoencoder learner and tunewrapper
lrn.h2o = makeLearner("oneclass.h2o.autoencoder", predict.type = "prob", activation = "Tanh", 
  reproducible = TRUE, mini_batch_size = 4, loss = "Quadratic", adaptive_rate = TRUE, l2=1e-5, 
  sparse = TRUE, max_w2=10, train_samples_per_iteration = -1)
lrn.h2o = makeTuneWrapper(lrn.h2o, resampling = inner, par.set = ps.h2o, measures = list(f1),
  control = ctrl, show.info = FALSE)

lrns = list(lrn.svm, lrn.ksvm, lrn.h2o, lrn.lofactor)
res = benchmark(lrns, oneclass2d.task, rin, measures = list(f1))
bmr = getBMRPerformances(res, as.df = TRUE)
bmr
```

## References
Mennatallah Amer, Markus Goldstein, and Slim Abdennadher. Enhancing one-class
support vector machines for unsupervised anomaly detection. In Proceedings of
the ACM SIGKDD Workshop on Outlier Detection and Description, pages 8–15.
ACM, 2013.

Varun Chandola, Arindam Banerjee, and Vipin Kumar. Anomaly detection: A survey.
ACM Comput. Surv., 41(3):15:1–15:58, July 2009. ISSN 0360-0300. doi: 10.
1145/1541880.1541882. URL http://doi.acm.org/10.1145/1541880.1541882.

Anh Hoang Dau, Victor Ciesielski, and Andy Song. Anomaly detection using replicator
neural networks trained on examples of one class. In SEAL, pages 311–322,
2014.

Michael R Smith and Tony Martinez. Improving classification accuracy by identifying
and removing instances that should be misclassified. In Neural Networks
(IJCNN), The 2011 International Joint Conference on, pages 2690–2697. IEEE,
2011.

Victoria Hodge and Jim Austin. A survey of outlier detection methodologies.
Artificial intelligence review, 22(2):85–126, 2004.

Vipin Kumar. Parallel and distributed computing for cybersecurity. IEEE
Distributed Systems Online, 6(10), 2005.

Alexandros Karatzoglou, Alex Smola, Kurt Hornik, and Achim Zeileis. kernlab – an
S4 package for kernel methods in R. Journal of Statistical Software, 11(9):1–20,
2004. URL http://www.jstatsoft.org/v11/i09/.

Guilherme O Campos, Arthur Zimek, Jörg Sander, Ricardo JGB Campello, Barbora
Micenková, Erich Schubert, Ira Assent, and Michael E Houle. On the evaluation
of unsupervised outlier detection: measures, datasets, and an empirical study.
Data Mining and Knowledge Discovery, 30(4):891–927, 2016.
    
Albert Thomas, Stephan Clemencon, Feuilard Vincent, and Alexandre Gramfort.
Learning hyperparameters for unsupervised anomaly detection. Anomaly
Detection Workshop, ICML 2016, 2016.

Nicolas Goix. How to evaluate the quality of unsupervised anomaly detection algorithms?
arXiv preprint arXiv:1607.01152, 2016.
