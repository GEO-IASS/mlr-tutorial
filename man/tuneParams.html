<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>R: Hyperparameter tuning.</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<link rel="stylesheet" type="text/css" href="R.css">

<link rel="stylesheet" href="http://yandex.st/highlightjs/7.3/styles/github.min.css">
<script src="http://yandex.st/highlightjs/7.3/highlight.min.js"></script>
<script src="http://yandex.st/highlightjs/7.3/languages/r.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
</head><body>

<table width="100%" summary="page for tuneParams {mlr}"><tr><td>tuneParams {mlr}</td><td align="right">R Documentation</td></tr></table>

<h2>Hyperparameter tuning.</h2>

<h3>Description</h3>

<p>Optimizes the hyperparameters of a learner.
Allows for different optimization methods, such as grid search, evolutionary strategies,
iterated F-race, etc. You can select such an algorithm (and its settings)
by passing a corresponding control object. For a complete list of implemented algorithms look at
<code><a href="TuneControl.html">TuneControl</a></code>.
</p>
<p>Multi-criteria tuning can be done with <code><a href="tuneParamsMultiCrit.html">tuneParamsMultiCrit</a></code>.
</p>


<h3>Usage</h3>

<pre>
tuneParams(learner, task, resampling, measures, par.set, control,
  show.info = getMlrOption("show.info"))
</pre>


<h3>Arguments</h3>

<table summary="R argblock">
<tr valign="top"><td><code>learner</code></td>
<td>
<p>[<code><a href="makeLearner.html">Learner</a></code> | <code>character(1)</code>]<br>
The learner.
If you pass a string the learner will be created via <code><a href="makeLearner.html">makeLearner</a></code>.</p>
</td></tr>
<tr valign="top"><td><code>task</code></td>
<td>
<p>[<code><a href="Task.html">Task</a></code>]<br>
The task.</p>
</td></tr>
<tr valign="top"><td><code>resampling</code></td>
<td>
<p>[<code><a href="makeResampleInstance.html">ResampleInstance</a></code> | <code><a href="makeResampleDesc.html">ResampleDesc</a></code>]<br>
Resampling strategy to evaluate points in hyperparameter space. If you pass a description,
it is instantiated once at the beginning by default, so all points are
evaluated on the same training/test sets.
If you want to change that behavior, look at <code><a href="TuneControl.html">TuneControl</a></code>.</p>
</td></tr>
<tr valign="top"><td><code>measures</code></td>
<td>
<p>[list of <code><a href="makeMeasure.html">Measure</a></code> | <code><a href="makeMeasure.html">Measure</a></code>]<br>
Performance measures to evaluate. The first measure, aggregated by the first aggregation function
is optimized, others are simply evaluated.</p>
</td></tr>
<tr valign="top"><td><code>par.set</code></td>
<td>
<p>[<code><a href="../../ParamHelpers/html/ParamSet.html">ParamSet</a></code>]<br>
Collection of parameters and their constraints for optimization.</p>
</td></tr>
<tr valign="top"><td><code>control</code></td>
<td>
<p>[<code><a href="TuneControl.html">TuneControl</a></code>]<br>
Control object for search method. Also selects the optimization algorithm for tuning.</p>
</td></tr>
<tr valign="top"><td><code>show.info</code></td>
<td>
<p>[<code>logical(1)</code>]<br>
Print verbose output on console?
Default is set via <code><a href="configureMlr.html">configureMlr</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>[<code><a href="TuneResult.html">TuneResult</a></code>].
</p>


<h3>See Also</h3>

<p>Other tune: <code><a href="makeModelMultiplexer.html">ModelMultiplexer</a></code>,
<code><a href="makeModelMultiplexer.html">makeModelMultiplexer</a></code>;
<code><a href="TuneControl.html">TuneControl</a></code>,
<code><a href="TuneControl.html">TuneControlCMAES</a></code>,
<code><a href="TuneControl.html">TuneControlGenSA</a></code>,
<code><a href="TuneControl.html">TuneControlGrid</a></code>,
<code><a href="TuneControl.html">TuneControlIrace</a></code>,
<code><a href="TuneControl.html">TuneControlRandom</a></code>,
<code><a href="TuneControl.html">makeTuneControlCMAES</a></code>,
<code><a href="TuneControl.html">makeTuneControlGenSA</a></code>,
<code><a href="TuneControl.html">makeTuneControlGrid</a></code>,
<code><a href="TuneControl.html">makeTuneControlIrace</a></code>,
<code><a href="TuneControl.html">makeTuneControlRandom</a></code>;
<code><a href="getTuneResult.html">getTuneResult</a></code>;
<code><a href="makeModelMultiplexerParamSet.html">makeModelMultiplexerParamSet</a></code>;
<code><a href="makeTuneWrapper.html">makeTuneWrapper</a></code>;
<code><a href="tuneThreshold.html">tuneThreshold</a></code>
</p>


<h3>Examples</h3>

<pre><code class="r"># a grid search for an SVM (with a tiny number of points...)
# note how easily we can optimize on a log-scale
ps = makeParamSet(
  makeNumericParam(&quot;C&quot;, lower = -12, upper = 12, trafo = function(x) 2^x),
  makeNumericParam(&quot;sigma&quot;, lower = -12, upper = 12, trafo = function(x) 2^x)
)
ctrl = makeTuneControlGrid(resolution = 2L)
rdesc = makeResampleDesc(&quot;CV&quot;, iters = 2L)
res = tuneParams(&quot;classif.ksvm&quot;, iris.task, rdesc, par.set = ps, control = ctrl)
</code></pre>

<pre><code>## [Tune] Started tuning learner classif.ksvm for parameter set:
##          Type len Def    Constr Req Trafo
## C     numeric   -   - -12 to 12   -     Y
## sigma numeric   -   - -12 to 12   -     Y
## With control class: TuneControlGrid
## Imputation value: 1
## [Tune] 1: C=0.000244; sigma=0.000244 : mmce.test.mean=0.72
## [Tune] 2: C=4.1e+03; sigma=0.000244 : mmce.test.mean=0.04
## [Tune] 3: C=0.000244; sigma=4.1e+03 : mmce.test.mean=0.72
## [Tune] 4: C=4.1e+03; sigma=4.1e+03 : mmce.test.mean=0.707
## [Tune] Result: C=4.1e+03; sigma=0.000244 : mmce.test.mean=0.04
</code></pre>

<pre><code class="r">print(res)
</code></pre>

<pre><code>## Tune result:
## Op. pars: C=4.1e+03; sigma=0.000244
## mmce.test.mean=0.04
</code></pre>

<pre><code class="r">print(as.data.frame(res$opt.path))
</code></pre>

<pre><code>##     C sigma mmce.test.mean dob eol error.message exec.time
## 1 -12   -12         0.7200   1  NA          &lt;NA&gt;     0.038
## 2  12   -12         0.0400   2  NA          &lt;NA&gt;     0.034
## 3 -12    12         0.7200   3  NA          &lt;NA&gt;     0.036
## 4  12    12         0.7067   4  NA          &lt;NA&gt;     0.036
</code></pre>

<pre><code class="r">print(as.data.frame(trafoOptPath(res$opt.path)))
</code></pre>

<pre><code>##           C     sigma mmce.test.mean dob eol
## 1 2.441e-04 2.441e-04         0.7200   1  NA
## 2 4.096e+03 2.441e-04         0.0400   2  NA
## 3 2.441e-04 4.096e+03         0.7200   3  NA
## 4 4.096e+03 4.096e+03         0.7067   4  NA
</code></pre>

<pre><code class="r">## Not run: 
##D # we optimize the SVM over 3 kernels simultanously
##D # note how we use dependent params (requires = ...) and iterated F-racing here
##D ps = makeParamSet(
##D   makeNumericParam(&quot;C&quot;, lower = -12, upper = 12, trafo = function(x) 2^x),
##D   makeDiscreteParam(&quot;kernel&quot;, values = c(&quot;vanilladot&quot;, &quot;polydot&quot;, &quot;rbfdot&quot;)),
##D   makeNumericParam(&quot;sigma&quot;, lower = -12, upper = 12, trafo = function(x) 2^x,
##D     requires = quote(kernel == &quot;rbfdot&quot;)),
##D   makeIntegerParam(&quot;degree&quot;, lower = 2L, upper = 5L,
##D     requires = quote(kernel == &quot;polydot&quot;))
##D )
##D print(ps)
##D ctrl = makeTuneControlIrace(maxExperiments = 200L)
##D rdesc = makeResampleDesc(&quot;Holdout&quot;)
##D res = tuneParams(&quot;classif.ksvm&quot;, iris.task, rdesc, par.set = ps, control = ctrl)
##D print(res)
##D print(head(as.data.frame(res$opt.path)))
## End(Not run)
</code></pre>


<hr><div align="center">[Package <em>mlr</em> version 2.1 <a href="00Index.html">Index</a>]</div>
</body></html>
