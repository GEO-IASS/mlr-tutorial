```{r include=FALSE}
# Not strictly necessary, but otherwise we might get NAs later on
## if 'rpart' is not installed.
library("rpart")
```

# Resampling

In order to assess the performance of a learning algorithm, resampling
strategies are usually used.
The entire data set is split into (multiple) training and test sets.
You train a learner on each training set, predict on the corresponding test set (sometimes
on the training set as well) and calculate some performance measure.
Then the individual performance values are aggregated, typically by calculating the mean.
There exist various different resampling strategies, for example
cross-validation and bootstrap, to mention just two popular approaches.

<!--(
.. |tune-varsel_processing| image:: /_images/resampling.png
     :align: middle
     :alt: Resampling Figure
     
)-->

If you want to read up further details, the paper
[Resampling Strategies for Model Assessment and Selection](http://link.springer.com/chapter/10.1007%2F978-0-387-47509-7_8)
by Simon is proabably not a bad choice.
Bernd has also published a paper
[Resampling methods for meta-model validation with recommendations for evolutionary computation](http://www.mitpressjournals.org/doi/pdf/10.1162/EVCO_a_00069)
which contains detailed descriptions and lots of statistical background information on resampling methods.

In [%mlr] the resampling strategy can be chosen via the function [&makeResampleDesc].
The supported resampling strategies are:

* Cross-validation (``"CV"``),
* Leave-one-out cross-validation (``"LOO""``),
* Repeated cross-validation (``"RepCV"``),
* Out-of-bag bootstrap and other variants (``"Bootstrap"``),
* Subsampling, also called Monte-Carlo cross-validaton (``"Subsample"``),
* Holdout (training/test) (``"Holdout"``).

The [&resample] function evaluates the performance of a [Learner](&makeLearner) using
the specified resampling strategy for a given machine learning [&Task].

In the following example the performance of the
[Cox proportional hazards model](&survival::coxph) on the
[lung](&survival::lung) data set is calculated using *3-fold cross-validation*.
Generally, in *K-fold cross-validation* the data set *D* is partitioned into *K* subsets of
(approximately) equal size.
In the *i*-th step of the *K* iterations, the *i*-th subset is
used for testing, while the union of the remaining parts forms the training
set.
The default performance measure in survival analysis is the concordance index ([cindex](measures.md)).

```{r}
## Specify the resampling strategy (3-fold cross-validation)
rdesc = makeResampleDesc("CV", iters = 3)

## Calculate the performance
r = resample("surv.coxph", lung.task, rdesc)
r
## peak a little bit into r
names(r)
r$aggr
r$measures.test
r$measures.train
```

``r$measures.test`` gives the value of the performance measure on the 3 individual test
data sets.
``r$aggr`` shows the aggregated performance value.
Its name, ``"cindex.test.mean"``, indicates the performance measure, [cindex](measures.md),
and the method used to aggregate the 3 individual performances.
[test.mean](&aggregations) is the default method and, as the name implies, takes the mean over the
performances on the 3 test data sets.
No predictions on the training data sets were made and thus ``r$measures.train`` contains missing values.

If predictions for the training set are required, too, set ``predict = "train"``or ``predict = "both"``
in [&makeResampleDesc]. This is necessary for some bootstrap methods (*b632* and *b632+*) and
we will see some examples later on.

``r$pred`` is an object of class [&ResamplePrediction].
Just as a [&Prediction] object (see the section on [making predictions](predict.md))
``r$pred`` has an element called ``"data"`` which is a ``data.frame`` that contains the
predictions and in case of a supervised learning problem the true values of the target
variable.

```{r}
head(r$pred$data)
```

The columns ``iter`` and ``set``indicate the resampling iteration and
if an individual prediction was made on the test or the training data set.


In the above example the performance measure is the concordance index ([cindex](measures.md)).
Of course, it is  possible to compute multiple performance measures at once by
passing a list of measures
(see also the previous section on [evaluating learner performance](performance.md)).

In the following we estimate the Dunn index ([dunn](measures.md)), the Davies-Bouldin cluster
separation measure ([db](measures.md)), and the time for training the learner ([timetrain](measures.md))
by *subsampling* with 5 iterations.
In each iteration the data set *D* is randomly partitioned into a
training and a test set according to a given percentage, e.g., 2/3
training and 1/3 test set. If there is just one iteration, the strategy
is commonly called *holdout* or *test sample estimation*.

```{r}
## cluster iris feature data
task = makeClusterTask(data = iris[,-5])
## Subsampling with 5 iterations and default split 2/3
rdesc = makeResampleDesc("Subsample", iters = 5)
## Subsampling with 5 iterations and 4/5 training data
rdesc = makeResampleDesc("Subsample", iters = 5, split = 4/5)

## Calculate the three performance measures
r = resample("cluster.kmeans", task, rdesc, measures = list(dunn, db, timetrain))
r$aggr
```


## Stratified resampling
For classification, it is usually desirable to have the same proportion of the classes in all of the partitions of the original data set. Stratified resampling ensures this.
This is particularly useful in case of imbalanced classes and small data sets. Otherwise it may happen, for example,
that observations of less frequent classes are missing in some of the training sets which can
decrease the performance of the learner, or lead to model crashes
In order to conduct stratified resampling, set ``stratify = TRUE`` when calling [&makeResampleDesc].

```{r}
## 3-fold cross-validation
rdesc = makeResampleDesc("CV", iters = 3, stratify = TRUE)

r = resample("classif.lda", iris.task, rdesc)
```

Stratification is also available for survival tasks.
Here the stratification balances the censoring rate.

Sometimes it is required to also stratify on the input data, e.g. to ensure that all subgroups are represented in all training and test sets.
To stratify on the input columns, specify factor columns of your task data via ``stratify.cols``
```{r}
rdesc = makeResampleDesc("CV", iters = 3, stratify.cols = "chas")
r = resample("regr.rpart", bh.task, rdesc)
```


## Accessing individual learner models

In each resampling iteration a [Learner](&makeLearner) is fitted on the respective training set.
By default, the resulting [WrappedModel](&makeWrappedModel)s are not returned by [&resample].
If you want to keep them, set ``models = TRUE`` when calling [&resample].

```{r}
## 3-fold cross-validation
rdesc = makeResampleDesc("CV", iters = 3)

r = resample("classif.lda", iris.task, rdesc, models = TRUE)
r$models
```

Keeping only certain information instead of entire [models](&makeWrappedModel), for example the
variable importance in a regression tree, can be achieved using the ``extract`` argument.
The function passed to ``extract`` is applied to each [model](&makeWrappedModel) fitted on one of
the 3 training sets.

```{r}
## 3-fold cross-validation
rdesc = makeResampleDesc("CV", iters = 3)

## Extract the variable importance in a regression tree
r = resample("regr.rpart", bh.task, rdesc,
    extract = function(x) x$learner.model$variable.importance)
r$extract
```


## Resample descriptions and resample instances

As shown above, the function [&makeResampleDesc] is used to specify the resampling strategy.

```{r}
rdesc = makeResampleDesc("CV", iters = 3)
str(rdesc)
```

The result ``rdesc``is an object of class [ResampleDesc](&makeResampleDesc) and contains,
as the name implies, a description of the resampling strategy.
In principle, this is an instruction for drawing training and test sets including
the necessary parameters like the number of iterations, the sizes of the training and test
sets etc.

Based on this description, the data set is randomly partitioned into multiple training and
test sets.
For each iteration, we get a set of index vectors indicating the training and test examples.
These are stored in a [ResampleInstance](&makeResampleInstance).

If a [ResampleDesc](&makeResampleDesc) is passed to [&resample], it is instantiated internally.
Naturally, it is also possible to pass a [ResampleInstance](&makeResampleInstance) directly.

A [ResampleInstance](&makeResampleInstance) can be created through the function
[&makeResampleInstance] given a [ResampleDesc](&makeResampleDesc) and either the size of
the data set at hand or the [&Task].
It basically performs the random drawing of indices to separate the data into training and
test sets according to the description.

```{r}
## Create a resample instance based an a task
rin = makeResampleInstance(rdesc, task = iris.task)
rin

## Create a resample instance given the size of the data set
rin = makeResampleInstance(rdesc, size = nrow(iris))
str(rin)

## Access the indices of the training observations in iteration 3
rin$train.inds[[3]]
```

While having two separate objects, resample descriptions and instances as well as the [&resample]
function seems overly complicated, it has several advantages:

* Resample instances allow for paired experiments, that is comparing the performance
  of several learners on exactly the same training and test sets.
  This is particularly useful if you want to add another method to a comparison experiment
  you already did.

```{r}
rdesc = makeResampleDesc("CV", iters = 3)
rin = makeResampleInstance(rdesc, task = iris.task)

## Calculate the performance of two learners based on the same resample instance
r.lda = resample("classif.lda", iris.task, rin, show.info = FALSE)
r.rpart = resample("classif.rpart", iris.task, rin, show.info = FALSE)
r.lda$aggr
r.rpart$aggr
```

* It is easy to add other resampling methods later on. You can
  simply derive from the [ResampleInstance](&makeResampleInstance)
  class, but you do not have to touch any methods that use the
  resampling strategy.


As mentioned above, when calling [&makeResampleInstance] the index sets are drawn randomly.
Mainly for *holdout* (*test sample*) *estimation* you might want full control about the training
and tests set and specify them manually.
This can be done using the function [&makeFixedHoldoutInstance].

```{r}
rin = makeFixedHoldoutInstance(train.inds = 1:100, test.inds = 101:150, size = 150)
rin
```


## Aggregating performance values

In resampling we get (for each measure we wish to calculate) one performance
value (on the test set, training set, or both) for each iteration.
Subsequently, these are aggregated.
As mentioned above, mainly the mean over the performance values on the test data sets
([test.mean](&aggregations)) is calculated.

For example, a 10-fold cross validation computes 10 values for the chosen
performance measure.
The aggregated value is the mean of these 10 numbers.
[%mlr] knows how to handle it because each [Measure](&makeMeasure) knows how it is aggregated:

```{r}
## Mean misclassification error
mmce$aggr

## Root mean square error
rmse$aggr
```

The aggregation method of a [Measure](&makeMeasure) can be changed via the function [&setAggregation].
See the documentation of [&aggregations] for available methods.

### Example: Different measures and aggregations

[test.median](&aggregations) computes the median of the performance values on the test sets.

```{r}
## We use the mean error rate and the median of the true positive rates
m1 = mmce
m2 = setAggregation(tpr, test.median)
rdesc = makeResampleDesc("CV", iters = 3)
r = resample("classif.rpart", sonar.task, rdesc, measures = list(m1, m2))
r$aggr
```

### Example: Calculating the training error

Here we calculate the mean misclassification error ([mmce](measures.md)) on the training and the test
data sets. Note that we have to set ``predict = "both"``when calling [&makeResampleDesc]
in order to get predictions on both data sets, training and test.

```{r}
mmce.train.mean = setAggregation(mmce, train.mean)
rdesc = makeResampleDesc("CV", iters = 3, predict = "both")
r = resample("classif.rpart", iris.task, rdesc, measures = list(mmce, mmce.train.mean))
r$measures.train
r$aggr
```


### Example: Bootstrap

In *out-of-bag bootstrap estimation* *B* new data sets *D_1* to *D_B* are drawn from the
data set *D* with replacement, each of the same size as *D*.
In the *i*-th iteration, *D_i* forms the training set, while the remaining elements from
*D*, i.e., elements not in the training set, form the test set.

<!--(
                     |resampling_desc_figure|

                     |resampling_nested_resampling_figure|
)-->

The variants *b632* and *b632+* calculate a convex combination of the training performance and
the out-of-bag bootstrap performance and thus require predictions on the training sets and an
appropriate aggregation strategy.


```{r}
rdesc = makeResampleDesc("Bootstrap", predict = "both", iters = 10)
b632.mmce = setAggregation(mmce, b632)
b632plus.mmce = setAggregation(mmce, b632plus)
b632.mmce

r = resample("classif.rpart", iris.task, rdesc,
    measures = list(mmce, b632.mmce, b632plus.mmce), show.info = FALSE)
head(r$measures.train)
r$aggr
```


## Convenience functions

When quickly trying out some learners, it can get tedious to write the **R**
code for generating a resample instance, setting the aggregation strategy and so
on. For this reason [%mlr] provides some convenience functions for the
frequently used resampling strategies, for example [holdout](&resample),
[crossval](&resample) or [bootstrapB632](&resample). But note that you do not
have as much control and flexibility as when using [&resample] with a resample
description or instance.

```{r, eval = FALSE}
holdout("regr.lm", bh.task, measures = list(mse, mae))
crossval("classif.lda", iris.task, iters = 3, measures = list(mmce, ber))
```
