# Training a Learner

Training a learner means fitting a model to a given data set.
In [%mlr] this can be done by calling the function [&train]
on a [Learner](&makeLearner) and a suitable [&Task].

Training a learner works the same way for every type of [&Task].
Here is a classification example using the data set [iris](&dataset::iris) and an LDA learner.

```{r}
lrn = makeLearner("classif.lda")
mod = train(lrn, iris.task)
mod
```

In the above example, creating a [Learner](&makeLearner) explicitly is not strictly necessary.
As a general rule, you have to create a [Learner](&makeLearner) if you want to change any defaults
by, e.g., setting hyperparameter values or changing the type of prediction.
Otherwise, [&train] and many other functions accept a character string naming the learning
method.

```{r}
mod = train("classif.lda", iris.task)
mod
```

Optionally, only a subset of the data, specified by an index set, can be used to
train the learner. This set is passed using the ``subset`` argument of [&train].

We fit a simple linear regression model to the [BostonHousing](&mlbench::BostonHousing) data set.
The object [&bh.task] is the regression [&Task] on the [BostonHousing](&mlbench::BostonHousing)
data set provided by [%mlr].

```{r}
## Number of observations
n = getTaskSize(bh.task)
## Use 1/3 of the observations for training
train.set = sample(n, size = n/3)
## Train the learner
mod = train("regr.lm", bh.task, subset = train.set)
mod
```

Note, for later, that all standard [resampling strategies](resample.md) are supported.
Therefore you usually do not have to subset the data yourself.

Moreover, if the [Learner](&makeLearner) supports this, you can specify observation ``weights``
that reflect the relevance of examples in the training process.

For example, in the [BreastCancer](&mlbench::BreastCancer) data set class ``benign`` is almost
twice as frequent as class malignant.
If both classes should have equal importance in training the classifier we can weight the
examples according to the class frequencies in the data set as shown in the following
**R** code (see also the section about [imbalanced classification problems](over_and_undersampling.md)).

```{r}
## Calculate the observation weights
target = getTaskTargets(bc.task)
tab = as.numeric(table(target))
w = 1/tab[target]

train("classif.rpart", task = bc.task, weights = w)
```

As you may recall, it is also possible to pass observation weights when creating the
[&Task].
Naturally, it makes sense to specify ``weights`` in [make<type>Task](&Task) if those weights
should always be used for the learning task and in [&train] if this is not the case since, e.g.,
some of the learners you want to use cannot deal with weights or you want to try different weights.
The weights in [&train] overwrite the weights in [&Task].
As a side remark for more advanced readers:
By varying the weights in the calls to train, you could also implement your own variant of
a general boosting type of algorithm on arbitrary mlr base learners.


Let's finish with a survival analysis example and train a
[Cox proportional hazards model](&survival::coxph)
on the [lung](&survival::lung) data set.

```{r}
data(lung, package = "survival")
lung$status = (lung$status == 2)
task = makeSurvTask(data = lung, target = c("time", "status"))
lrn = makeLearner("surv.coxph")

mod = train(lrn, task)
mod
```

## Wrapped models
[&train] returns an object of class [WrappedModel](&makeWrappedModel), which wraps the
particular model of the underlying **R** learning method.
This object contains the actual fitted model fit as returned by the **R** external package and additionally some informations about the learner and task.
It can subsequently be used to perform a [prediction](&predict.WrappedModel) for new observations.

In order to access the underlying model we can use the function [&getLearnerModel].
In the following example we get an object of class [lm](&stats::lm).

```{r}
mod = train("regr.lm", bh.task, subset = train.set)
getLearnerModel(mod)
```
